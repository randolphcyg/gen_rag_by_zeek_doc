import os
import time
import json
import requests
import logging
from tqdm import tqdm
from pymilvus import MilvusClient, DataType
from pymilvus.milvus_client import IndexParams

# ===================== æ—¥å¿—é…ç½® =====================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# ===================== æ ¸å¿ƒé…ç½® =====================
MILVUS_HOST = "localhost"
MILVUS_PORT = 19530
COLLECTION_NAME = "zeek_rag_v8_0_4"  # å»ºè®®ç‰ˆæœ¬å·å…¥åº“å

JSON_FILE_PATH = r"G:\share\goodjob\gen_rag_by_zeek_doc\modify_zeek_rag.json"

OLLAMA_HOST = "http://localhost:11434"
OLLAMA_MODEL = "nomic-embed-text:latest"
EMBEDDING_DIM = 768

BATCH_SIZE_EMBEDDING = 8
BATCH_SIZE_MILVUS = 200

# å­—èŠ‚æ•°é™åˆ¶ï¼ˆMilvus VARCHAR ä»¥å­—èŠ‚è®¡ï¼‰
MAX_BYTES_RAW = 8000
MAX_BYTES_CLEAN = 6000

# ===================== å¤„ç†ç±» =====================
class ZeekMilvusPusher:

    def __init__(self):
        self.milvus_client = MilvusClient(uri=f"http://{MILVUS_HOST}:{MILVUS_PORT}")
        self._check_ollama_health()

    def _check_ollama_health(self):
        try:
            resp = requests.get(f"{OLLAMA_HOST}/api/tags", timeout=5)
            resp.raise_for_status()
        except:
            raise RuntimeError(f"Ollama æœåŠ¡æœªå¯åŠ¨æˆ–æ— æ³•è¿æ¥: {OLLAMA_HOST}")

    def create_collection(self):
        if self.milvus_client.has_collection(COLLECTION_NAME):
            logger.warning(f"åˆ é™¤æ—§é›†åˆ: {COLLECTION_NAME}")
            self.milvus_client.drop_collection(COLLECTION_NAME)

        schema = self.milvus_client.create_schema(auto_id=True, primary_field_name="pk")

        # å®šä¹‰å­—æ®µ
        schema.add_field("pk", DataType.INT64, is_primary=True)
        schema.add_field("partition_tag", DataType.VARCHAR, max_length=50) # p_logs, p_reference ç­‰
        schema.add_field("doc_id", DataType.VARCHAR, max_length=500)
        schema.add_field("doc_title", DataType.VARCHAR, max_length=500)
        schema.add_field("section_title", DataType.VARCHAR, max_length=500)
        schema.add_field("content_type", DataType.VARCHAR, max_length=50)  # text, code, symbol
        schema.add_field("raw_content", DataType.VARCHAR, max_length=MAX_BYTES_RAW)
        schema.add_field("embedding", DataType.FLOAT_VECTOR, dim=EMBEDDING_DIM)
        schema.add_field("update_time", DataType.INT64)

        index_params = self.milvus_client.prepare_index_params()
        index_params.add_index(
            field_name="embedding",
            index_type="IVF_FLAT",
            metric_type="COSINE",
            params={"nlist": 128}
        )

        self.milvus_client.create_collection(
            collection_name=COLLECTION_NAME,
            schema=schema,
            index_params=index_params
        )
        logger.info(f"æˆåŠŸåˆ›å»º Collection: {COLLECTION_NAME}")

    def _safe_truncate(self, text: str, max_bytes: int) -> str:
        if not text: return ""
        text = text.replace("\x00", "").strip()
        encoded = text.encode("utf-8")
        if len(encoded) <= max_bytes:
            return text
        return encoded[:max_bytes].decode("utf-8", errors="ignore")

    def _get_embedding(self, texts: list):
        # æ‰¹é‡è·å– Embedding
        resp = requests.post(
            f"{OLLAMA_HOST}/api/embed",
            json={"model": OLLAMA_MODEL, "input": texts},
            timeout=120
        )
        resp.raise_for_status()
        return resp.json()["embeddings"]

    def _iter_sections(self, sections, parents=None):
        if parents is None: parents = []
        for sec in sections:
            full_title = " > ".join(parents + [sec["title"]])
            # æå–æœ¬çº§ blocks
            for block in sec.get("blocks", []):
                yield full_title, block
            # é€’å½’å­çº§
            if sec.get("subsections"):
                yield from self._iter_sections(sec["subsections"], parents + [sec["title"]])

    def process(self):
        with open(JSON_FILE_PATH, "r", encoding="utf-8") as f:
            all_docs = json.load(f)

        pending_records = []

        for doc in tqdm(all_docs, desc="è§£ææ–‡æ¡£å†…å®¹"):
            doc_meta = {
                "partition_tag": doc.get("partition", "p_guides"),
                "doc_id": doc["doc_id"],
                "doc_title": doc["title"],
                "update_time": int(time.time())
            }

            # 1. å¤„ç†ç« èŠ‚ä¸­çš„å†…å®¹
            for sec_title, block in self._iter_sections(doc["sections"]):
                content = ""
                if block["type"] == "code":
                    content = f"Code block ({block.get('language','')}):\n{block.get('code','')}"
                else:
                    content = block.get("text", "")

                if not content or len(content) < 10: continue

                pending_records.append({
                    **doc_meta,
                    "section_title": sec_title,
                    "content_type": block["type"],
                    "raw_content": self._safe_truncate(content, MAX_BYTES_RAW)
                })

            # 2. å¤„ç†ç‹¬ç«‹çš„ Symbols (é«˜ä»·å€¼ API å®šä¹‰)
            for sym in doc.get("symbols", []):
                sym_text = f"Zeek {sym['symbol_type']} definition: {sym['text']}"
                pending_records.append({
                    **doc_meta,
                    "section_title": sym.get("section", "API Reference"),
                    "content_type": "symbol",
                    "raw_content": self._safe_truncate(sym_text, MAX_BYTES_RAW)
                })

        logger.info(f"è§£æå®Œæˆï¼Œå‡†å¤‡ç”Ÿæˆå‘é‡å¹¶å…¥åº“ï¼Œæ€» Chunk æ•°: {len(pending_records)}")

        # 3. æ‰¹é‡å‘é‡åŒ–å¹¶å…¥åº“
        for i in tqdm(range(0, len(pending_records), BATCH_SIZE_MILVUS), desc="å…¥åº“è¿›åº¦"):
            batch = pending_records[i : i + BATCH_SIZE_MILVUS]

            # Ollama æ‰¹é‡ Embedding é€»è¾‘ (é’ˆå¯¹ batch å†…éƒ¨å†æ¬¡åˆ‡åˆ†é¿å…è¶…è½½)
            texts_to_embed = [r["raw_content"][:3000] for r in batch] # é™åˆ¶ embedding æ–‡æœ¬é•¿åº¦

            try:
                embeddings = []
                for j in range(0, len(texts_to_embed), BATCH_SIZE_EMBEDDING):
                    sub_batch = texts_to_embed[j : j + BATCH_SIZE_EMBEDDING]
                    embeddings.extend(self._get_embedding(sub_batch))

                for record, emb in zip(batch, embeddings):
                    record["embedding"] = emb

                self.milvus_client.insert(collection_name=COLLECTION_NAME, data=batch)
            except Exception as e:
                logger.error(f"æ‰¹é‡å…¥åº“å¤±è´¥: {e}")

        logger.info("ğŸ‰ æ‰€æœ‰æ•°æ®å·²æ¨é€è‡³ Milvus!")

if __name__ == "__main__":
    pusher = ZeekMilvusPusher()
    pusher.create_collection()
    pusher.process()