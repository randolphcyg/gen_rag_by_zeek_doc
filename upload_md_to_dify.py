import json
import requests
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

# ===================== é…ç½®åŒº =====================

DIFY_API_BASE = "http://localhost:5001/v1"
API_KEY = "dataset-MF0p7JRI8hUO5nHXRJ73szfi"
DATASET_ID = "ec367307-db47-4449-9624-6e8ae9d6c405"

# è‡ªåŠ¨å®šä½å½“å‰è„šæœ¬åŒçº§çš„ flattened ç›®å½•
BASE_DIR = Path(__file__).parent.absolute()
DOCS_DIR = BASE_DIR / "zeek_docs_flattened"

# å¹¶å‘æ•° (å»ºè®® 4-8ï¼Œè¿‡é«˜ä¼šå¯¼è‡´ Dify æˆ– æ•°æ®åº“ æŠ¥é”™)
MAX_WORKERS = 8

# ===================== çˆ¶å­ç´¢å¼•è§„åˆ™ =====================

PROCESS_RULE = {
    "mode": "hierarchical",
    "rules": {
        "pre_processing_rules": [
            {"id": "remove_extra_spaces", "enabled": True},
            {"id": "remove_urls_emails", "enabled": False}
        ],
        "segmentation": {
            "separator": "\n### ",
            "max_tokens": 1500,
            "chunk_overlap": 50
        },
        "parent_child_indexing": {
            "enabled": True,
            "child_chunk_size": 400,
            "child_chunk_overlap": 100
        }
    }
}

# ===================== æ ¸å¿ƒé€»è¾‘ =====================

def upload_single_file(filepath: Path):
    """
    å•ä¸ªæ–‡ä»¶ä¸Šä¼ é€»è¾‘
    """
    url = f"{DIFY_API_BASE}/datasets/{DATASET_ID}/document/create_by_file"
    headers = {"Authorization": f"Bearer {API_KEY}"}

    # ç›´æ¥ä½¿ç”¨æ–‡ä»¶å (å› ä¸ºä¹‹å‰å·²ç»å¤„ç†è¿‡å®‰å…¨é•¿åº¦äº†)
    filename = filepath.name

    data = {
        "indexing_technique": "high_quality",
        "process_rule": json.dumps(PROCESS_RULE),
        "doc_form": "text_model",  # æ ‡å‡†æ¨¡å¼ï¼Œå…·ä½“çš„å±‚çº§ç”± process_rule å†³å®š
        "doc_language": "English"
    }

    try:
        with open(filepath, 'rb') as f:
            files = {'file': (filename, f, 'text/markdown')}
            # è®¾ç½® timeout é˜²æ­¢ç½‘ç»œå¡æ­»
            resp = requests.post(url, headers=headers, data=data, files=files, timeout=60)

            if resp.status_code in [200, 201]:
                return True, filename, ""
            else:
                return False, filename, f"Status {resp.status_code}: {resp.text[:100]}"
    except Exception as e:
        return False, filename, str(e)

def main():
    if not DOCS_DIR.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {DOCS_DIR}")
        print("   è¯·ç¡®ä¿ä½ å·²ç»è¿è¡Œäº†æ„å»ºè„šæœ¬ï¼Œå¹¶ä¸”æ–‡ä»¶å¤¹åœ¨å½“å‰è„šæœ¬æ—è¾¹ã€‚")
        return

    # æ‰«æç›®å½•ä¸‹æ‰€æœ‰çš„ md æ–‡ä»¶ (æ‰å¹³ç»“æ„ä¸éœ€è¦ recursive)
    files = list(DOCS_DIR.glob("*.md"))
    total_files = len(files)

    if total_files == 0:
        print("âŒ ç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ° .md æ–‡ä»¶")
        return

    print(f"ğŸ“¦ å‡†å¤‡ä¸Šä¼  {total_files} ä¸ªæ–‡æ¡£")
    print(f"ğŸš€ å¹¶å‘çº¿ç¨‹: {MAX_WORKERS}")
    print("-" * 40)

    success_count = 0
    fail_count = 0

    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘ä¸Šä¼ 
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # æäº¤ä»»åŠ¡
        future_to_file = {executor.submit(upload_single_file, f): f for f in files}

        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
        pbar = tqdm(as_completed(future_to_file), total=total_files, unit="doc")

        for future in pbar:
            success, name, error_msg = future.result()

            if success:
                success_count += 1
            else:
                fail_count += 1
                # åªæœ‰å¤±è´¥æ—¶æ‰æ‰“å°è¯¦ç»†ä¿¡æ¯ï¼Œé¿å…åˆ·å±
                tqdm.write(f"âŒ å¤±è´¥: {name} | åŸå› : {error_msg}")

    print("\n" + "="*40)
    print(f"ğŸ‰ å…¨éƒ¨å®Œæˆ!")
    print(f"âœ… æˆåŠŸ: {success_count}")
    print(f"âŒ å¤±è´¥: {fail_count}")

if __name__ == "__main__":
    main()