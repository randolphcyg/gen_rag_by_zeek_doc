import os
import json
import requests
import time
import hashlib
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

# ===================== é…ç½®åŒº =====================

DIFY_API_BASE = "http://localhost:5001/v1"
API_KEY = "dataset-MF0p7JRI8hUO5nHXRJ73szfi"
DATASET_ID = "60f859e2-3143-48a9-bbb9-d6d1e5136f26"
DOCS_DIR = r"E:\share\goodjob\gen_rag_by_zeek_doc\zeek_docs_markdown"

MAX_WORKERS = 8

# æ•°æ®åº“é€šå¸¸é™åˆ¶ 255ï¼Œæˆ‘ä»¬è®¾å®šå®‰å…¨é˜ˆå€¼ 240
MAX_FILENAME_LEN = 240

# ===================== æ ¸å¿ƒé€»è¾‘ =====================

PROCESS_RULE = {
    "mode": "hierarchical",
    "rules": {
        "pre_processing_rules": [
            {"id": "remove_extra_spaces", "enabled": True},
            {"id": "remove_urls_emails", "enabled": False}
        ],
        "segmentation": {
            "separator": "\n### ",
            "max_tokens": 1500,
            "chunk_overlap": 50
        },
        "parent_child_indexing": {
            "enabled": True,
            "child_chunk_size": 400,
            "child_chunk_overlap": 100
        }
    }
}

def get_safe_filename(filepath: Path, root_dir: Path) -> str:
    """
    ç”Ÿæˆç¬¦åˆé•¿åº¦é™åˆ¶çš„å”¯ä¸€æ–‡ä»¶å
    """
    try:
        # 1. å°è¯•ç”Ÿæˆå…¨è·¯å¾„å: dir_subdir_filename.md
        rel_path = filepath.relative_to(root_dir)
        full_name = str(rel_path).replace(os.sep, "_").replace("/", "_").replace("\\", "_")
    except ValueError:
        full_name = filepath.name

    # 2. æ£€æŸ¥é•¿åº¦
    name_len = len(full_name.encode('utf-8')) # ä½¿ç”¨ utf-8 å­—èŠ‚é•¿åº¦æ›´å‡†ç¡®

    # è°ƒè¯•æ‰“å°ï¼ˆåªåœ¨æ¥è¿‘è¶…é•¿æ—¶æ‰“å°ï¼Œé¿å…åˆ·å±ï¼‰
    if name_len > 200:
        print(f"âš ï¸ [é•¿åº¦é¢„è­¦] {name_len} chars: {full_name}")

    # 3. å¦‚æœè¶…é•¿ï¼Œè¿›è¡Œæ™ºèƒ½æˆªæ–­
    if name_len > MAX_FILENAME_LEN:
        # ç­–ç•¥ï¼šä¿ç•™æ–‡ä»¶åæœ¬èº«(è¯­ä¹‰) + è·¯å¾„çš„MD5å“ˆå¸Œ(å”¯ä¸€æ€§) + æ‰©å±•å
        # ä¾‹å¦‚ï¼šapi.zeek_a1b2c3d4.md
        ext = filepath.suffix  # .md
        stem = filepath.stem   # api.zeek

        # è®¡ç®—å®Œæ•´è·¯å¾„çš„ Hash (å–å‰8ä½)
        path_hash = hashlib.md5(str(rel_path).encode('utf-8')).hexdigest()[:8]

        # æ„é€ æ–°åå­—
        safe_name = f"{stem}_{path_hash}{ext}"

        # å¦‚æœè¿åŸæ–‡ä»¶åéƒ½å¾ˆé•¿ï¼Œå¯¼è‡´ safe_name ä¾ç„¶è¶…é•¿ï¼Œé‚£å°±åªä¿ç•™ Hash
        if len(safe_name.encode('utf-8')) > MAX_FILENAME_LEN:
            safe_name = f"doc_{path_hash}{ext}"

        print(f"âœ‚ï¸ [è‡ªåŠ¨æˆªæ–­] åŸé•¿ {name_len} -> æ–°å: {safe_name}")
        return safe_name

    return full_name

def upload_single_file(filepath: Path, root_dir: Path):
    url = f"{DIFY_API_BASE}/datasets/{DATASET_ID}/document/create_by_file"
    headers = {"Authorization": f"Bearer {API_KEY}"}

    # è·å–å®‰å…¨çš„æ–‡ä»¶å
    unique_name = get_safe_filename(filepath, root_dir)

    data = {
        "indexing_technique": "high_quality",
        "process_rule": json.dumps(PROCESS_RULE),
        "doc_form": "hierarchical_model",
        "doc_language": "English"
    }

    try:
        with open(filepath, 'rb') as f:
            files = {'file': (unique_name, f, 'text/markdown')}
            resp = requests.post(url, headers=headers, data=data, files=files, timeout=60)

            if resp.status_code in [200, 201]:
                return True, unique_name, ""
            else:
                return False, unique_name, f"Status {resp.status_code}: {resp.text}"
    except Exception as e:
        return False, unique_name, str(e)

def main():
    root_path = Path(DOCS_DIR)
    if not root_path.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {DOCS_DIR}")
        return

    files = list(root_path.glob("**/*.md"))
    total_files = len(files)

    print(f"ğŸ“¦ å‡†å¤‡å¹¶å‘ä¸Šä¼  {total_files} ä¸ªæ–‡æ¡£")
    print(f"ğŸ“ æœ€å¤§æ–‡ä»¶åé•¿åº¦é™åˆ¶: {MAX_FILENAME_LEN} å­—ç¬¦")
    print("-" * 40)

    success_count = 0
    fail_count = 0

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_file = {executor.submit(upload_single_file, f, root_path): f for f in files}

        pbar = tqdm(as_completed(future_to_file), total=total_files, unit="file")

        for future in pbar:
            success, name, error_msg = future.result()

            if success:
                success_count += 1
            else:
                fail_count += 1
                tqdm.write(f"âŒ å¤±è´¥: {name} | {error_msg}")

    print("\n" + "="*40)
    print(f"ğŸ‰ å¤„ç†å®Œæˆ | æˆåŠŸ: {success_count} | å¤±è´¥: {fail_count}")

if __name__ == "__main__":
    main()